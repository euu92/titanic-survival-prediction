# Titanic: Machine Learning for Survival Prediction ðŸš¢

Este es mi primer proyecto de Ciencia de Datos de principio a fin, basado en la clÃ¡sica competiciÃ³n de [Kaggle "Titanic - Machine Learning from Disaster"](https://www.kaggle.com/c/titanic).

El objetivo es construir un modelo de Machine Learning que prediga quÃ© pasajeros sobrevivieron al desastre del Titanic basÃ¡ndose en un conjunto de caracterÃ­sticas.

---

##  workflow del Proyecto

Mi proceso se dividiÃ³ en 6 fases clave, documentadas en el notebook `titanic-analysis.ipynb`:

### 1. Carga e InspecciÃ³n
* Carga de los `train.csv` y `test.csv`.
* InspecciÃ³n inicial (`.info()`, `.describe()`) para identificar tipos de datos, valores nulos (en `Age`, `Cabin`, `Embarked`) y estadÃ­sticas bÃ¡sicas.

### 2. AnÃ¡lisis Exploratorio de Datos (EDA)
* AnÃ¡lisis de correlaciÃ³n inicial usando `groupby()` para ver la tasa de supervivencia por `Pclass`, `Sex`, `SibSp` y `Parch`.
* Visualizaciones con `seaborn` para entender las distribuciones (ej. `Age vs. Survival`).

### 3. IngenierÃ­a de CaracterÃ­sticas (El Trabajo de "Detective")
Esta fue la fase mÃ¡s crÃ­tica. CreÃ© varias caracterÃ­sticas nuevas para mejorar la precisiÃ³n del modelo:
* `**Family_Size`**: Combinando `SibSp` y `Parch`.
* `**Family_Size_Grouped`**: Agrupando `Family_Size` en categorÃ­as Ãºtiles ('Alone', 'Small', 'Medium', 'Large').
* `**Age_Cut`**: Binarizando la columna `Age` en 8 grupos basados en `pd.qcut()`.
* `**Fare_Cut`**: Binarizando la columna `Fare` (muy sesgada) en 6 grupos.
* `**Title`**: ExtraÃ­do de la columna `Name` (ej. 'Mr', 'Mrs', 'Master') y agrupando tÃ­tulos raros ('Military', 'Noble').
* `**TicketNumberCounts`**: Calculando cuÃ¡ntos pasajeros compartÃ­an el mismo nÃºmero de ticket.
* `**Cabin_Assigned`**: Una caracterÃ­stica binaria (1 o 0) que indica si un pasajero tenÃ­a una `Cabin` asignada o era 'U' (Desconocido).

### 4. Preprocesamiento (La Pipeline)
* **Limpieza Final:** RellenÃ© los Ãºltimos valores nulos (ej. `Embarked` con la moda 'S', `Fare` con la mediana).
* **EliminaciÃ³n de Columnas:** EliminÃ© las columnas "crudas" que ya no eran necesarias (ej. `Name`, `Ticket`, `SibSp`).
* **Pipeline:** ConstruÃ­ un `ColumnTransformer` para automatizar todo el preprocesamiento, incluyendo:
    * `SimpleImputer()` para rellenar cualquier nulo restante.
    * `OneHotEncoder()` para convertir todas las caracterÃ­sticas categÃ³ricas (ej. `Sex`, `Title`) en nÃºmeros.

### 5. Entrenamiento y OptimizaciÃ³n de Modelos
* DividÃ­ los datos en `X_train`, `y_train` y `X_valid` para la validaciÃ³n.
* **ComparÃ© 6 modelos de clasificaciÃ³n diferentes**:
    1.  Random Forest
    2.  Decision Tree
    3.  K-Neighbors (KNN)
    4.  Support Vector (SVC)
    5.  Logistic Regression
    6.  Gaussian Naive Bayes
* UsÃ© `GridSearchCV` con `StratifiedKFold(n_splits=5)` en cada modelo para encontrar los mejores hiperparÃ¡metros y prevenir el sobreajuste.

### 6. Resultados y EnvÃ­o
* ComparÃ© las puntuaciones de *accuracy* de los 6 modelos optimizados.
* El modelo com mayor *accuracy* es *Random Forest*.
* GenerÃ© los archivos `submission[i].csv`.

---

## Resultados
La fase de *Feature Engineering* fue clave. CaracterÃ­sticas como `Title` y `Cabin_Assigned` demostraron ser predictores muy fuertes.

| Modelo | Mejor Accuracy (Cross-Validation) |
| :--- | :--- |
| **Random Forest** | **[0.83]** |
| Decision Tree | [0.8159] |
| K-Neighbors (KNN) | [0.8076] |
| Support Vector (SVC)| [0.7991] |
| Logistic Regression | [0.8048] |
| Gaussian Naive Bayes| [0.7795] |

---

## CÃ³mo Ejecutar
1.  Clona o descarga este repositorio.
2.  AsegÃºrate de tener las librerÃ­as necesarias (`pandas`, `numpy`, `sklearn`, `seaborn`, `matplotlib`).
3.  Abre `titanic-analysis.ipynb` en Jupyter Notebook y haz clic en "Kernel" -> "Restart & Run All".
